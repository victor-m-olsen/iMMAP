{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "#import gdal\n",
    "from osgeo import gdal\n",
    "from netCDF4 import Dataset\n",
    "from sqlalchemy import create_engine, text\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from ftplib import FTP\n",
    "from sqlalchemy.exc import SQLAlchemyError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_db_config(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def generate_file_path(base_path, date):\n",
    "    date_arr = date.split('-')\n",
    "    filename = f\"glofas_areagrid_for_IMMAP_in_Afghanistan_{date_arr[0]}{date_arr[1]}{date_arr[2]}00.nc\"\n",
    "    return os.path.join(base_path, filename)\n",
    "\n",
    "def download_nc_file(directory_path, date):\n",
    "    start_time = datetime.now()\n",
    "    print(f\"download_nc_file start time: {start_time}\")\n",
    "    date_arr = date.split('-')\n",
    "    filename = f\"glofas_areagrid_for_IMMAP_in_Afghanistan_{date_arr[0]}{date_arr[1]}{date_arr[2]}00.nc\"\n",
    "    local_path = os.path.join(directory_path, filename)\n",
    "\n",
    "    if os.path.exists(local_path):\n",
    "        print(f\"The latest Glofas file {filename} already exists.\")\n",
    "        return local_path\n",
    "    else:\n",
    "        print(f\"Downloading {filename} from FTP server...\")\n",
    "        # FTP server details\n",
    "        ftp_server = #\n",
    "        ftp_username = #  # Replace with actual username\n",
    "        ftp_password = #  # Replace with actual password\n",
    "        ftp_folder = #\n",
    "\n",
    "        try:\n",
    "            server = FTP(ftp_server)\n",
    "            server.login(ftp_username, ftp_password)\n",
    "            server.cwd(ftp_folder)\n",
    "\n",
    "            file_list = server.nlst()\n",
    "            if filename in file_list:\n",
    "                with open(local_path, \"wb\") as file:\n",
    "                    server.retrbinary(\"RETR \" + filename, file.write)\n",
    "                print(f\"File {filename} downloaded successfully.\")\n",
    "            else:\n",
    "                print(f\"The file {filename} does not exist on the FTP server.\")\n",
    "            server.quit()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {filename} from FTP server. Error: {e}\")\n",
    "        end_time = datetime.now()\n",
    "        print(f\"download_nc_file end time: {end_time}\")\n",
    "        print(f\"download_nc_file Duration: {end_time - start_time}\")\n",
    "    return local_path\n",
    "\n",
    "\n",
    "def initialize_paths(directory_path):\n",
    "    discharge_tif_paths = [os.path.join(directory_path, f'discharge_day{days}.tif') for days in ['1_3', '4_10', '11_30']]\n",
    "    alert_tif_paths = [os.path.join(directory_path, f'alert_day{days}.tif') for days in ['1_3', '4_10', '11_30']]\n",
    "    return discharge_tif_paths, alert_tif_paths\n",
    "\n",
    "def save_tif_file(array, output_path, geotransform, projection, datatype, no_data_value=None):\n",
    "    start_time = datetime.now()\n",
    "    print(f\"save_tif_file start time: {start_time}\")\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    y_size, x_size = array.shape\n",
    "    dataset = driver.Create(output_path, x_size, y_size, 1, datatype)\n",
    "    dataset.SetGeoTransform(geotransform)\n",
    "    dataset.SetProjection(projection)\n",
    "    band = dataset.GetRasterBand(1)\n",
    "    if no_data_value is not None:\n",
    "        band.SetNoDataValue(float(no_data_value))\n",
    "    band.WriteArray(array)\n",
    "    band.FlushCache()\n",
    "    dataset = None\n",
    "    end_time = datetime.now()\n",
    "    print(f\"save_tif_file end time: {end_time}\")\n",
    "    print(f\"save_tif_file Duration: {end_time - start_time}\")\n",
    "\n",
    "def create_alert_tif(discharge, return_level, output_path, gt, proj, no_data_value):\n",
    "    start_time = datetime.now()\n",
    "    print(f\"create_alert_tif start time: {start_time}\")\n",
    "    # Initialize an array with the no_data_value where the discharge is no data\n",
    "    alert_array = np.full(discharge.shape, no_data_value, dtype='float32')\n",
    "\n",
    "    # Apply alert conditions only where discharge data is valid\n",
    "    valid_data_mask = (discharge != no_data_value)\n",
    "    alert_conditions = np.where((discharge >= return_level) & valid_data_mask, 1, 0)\n",
    "    \n",
    "    # Place the alert conditions into the alert array, preserving no data values\n",
    "    alert_array[valid_data_mask] = alert_conditions[valid_data_mask]\n",
    "\n",
    "    # Save the alert array to a .tif file\n",
    "    save_tif_file(alert_array, output_path, gt, proj, gdal.GDT_Float32, no_data_value)\n",
    "    end_time = datetime.now()\n",
    "    print(f\"create_alert_tif end time: {end_time}\")\n",
    "    print(f\"create_alert_tif Duration: {end_time - start_time}\")\n",
    "\n",
    "def process_netcdf_data(input_file, time_ranges, discharge_tif_paths, alert_tif_paths, gt, proj, no_data_value):\n",
    "    start_time = datetime.now()\n",
    "    print(f\"process_netcdf_data start time: {start_time}\")\n",
    "    with Dataset(input_file, 'r') as nc:\n",
    "        dis_var = nc.variables['dis']\n",
    "        rl2 = nc.variables['rl2'][:]\n",
    "        dis_var_masked = np.ma.masked_values(dis_var[:], no_data_value)\n",
    "        for (start_day, end_day), discharge_path, alert_path in zip(time_ranges, discharge_tif_paths, alert_tif_paths):\n",
    "            average_discharge = np.ma.mean(dis_var_masked[:, start_day:end_day, :, :], axis=(0, 1))\n",
    "            average_discharge.set_fill_value(no_data_value)\n",
    "            save_tif_file(average_discharge.filled(), discharge_path, gt, proj, gdal.GDT_Float32, no_data_value)\n",
    "            create_alert_tif(average_discharge.filled(), rl2, alert_path, gt, proj, no_data_value)\n",
    "    end_time = datetime.now()\n",
    "    print(f\"process_netcdf_data end time: {end_time}\")\n",
    "    print(f\"process_netcdf_data Duration: {end_time - start_time}\")\n",
    "\n",
    "def update_glofas_points(conn, raster_paths, column_names, glofas_points):\n",
    "    start_time = datetime.now()\n",
    "    print(f\"update_glofas_points start time: {start_time}\")\n",
    "\n",
    "    for raster_path, column_name in zip(raster_paths, column_names):\n",
    "        with rasterio.open(raster_path) as src:\n",
    "            raster_array = src.read(1)\n",
    "            transform = src.transform\n",
    "\n",
    "            # Prepare batch update\n",
    "            updates = []\n",
    "            for index, row in glofas_points.iterrows():\n",
    "                row_x, row_y = row.geom.x, row.geom.y\n",
    "                row_col, row_row = ~transform * (row_x, row_y)\n",
    "                row_col, row_row = int(row_col), int(row_row)\n",
    "                raster_value = raster_array[row_row, row_col]\n",
    "                updates.append(f\"({raster_value}, {row['id_glofas']})\")\n",
    "\n",
    "            # Perform batch update\n",
    "            values_clause = ', '.join(updates)\n",
    "            update_query = f\"UPDATE glofas_points SET {column_name} = data.raster_value FROM (VALUES {values_clause}) AS data (raster_value, id_glofas) WHERE glofas_points.id_glofas = data.id_glofas\"\n",
    "            conn.execute(text(update_query))\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    print(f\"update_glofas_points end time: {end_time}\")\n",
    "    print(f\"update_glofas_points Duration: {end_time - start_time}\")\n",
    "\n",
    "\n",
    "# Update summary glofas join table (adm2-basin-flood polygons), and aggregate to adm2 and basin level \n",
    "def execute_sql_queries(conn):\n",
    "    start_time = datetime.now()\n",
    "    print(f\"execute_sql_queries start time: {start_time}\")\n",
    "    \n",
    "    conn.autocommit = True\n",
    "\n",
    "    # SQL query to update glofas_join\n",
    "    update_glofas_join = text(\"\"\"\n",
    "    UPDATE glofas_join b\n",
    "    SET alert_1_3 = g.alert_1_3,\n",
    "        alert_4_10 = g.alert_4_10,\n",
    "        alert_11_30 = g.alert_11_30\n",
    "    FROM glofas_points g\n",
    "    WHERE b.basin_id = g.id_basin;\n",
    "    \"\"\")\n",
    "\n",
    "    # SQL query to update data for afg_adm2_summary\n",
    "    update_adm2_query = text(\"\"\"\n",
    "    UPDATE afg_adm2_summary a\n",
    "    SET pop_1_3 = sub.pop_1_3,\n",
    "        pop_4_10 = sub.pop_4_10,\n",
    "        pop_11_30 = sub.pop_11_30,\n",
    "        build_1_3 = sub.build_1_3,\n",
    "        build_4_10 = sub.build_4_10,\n",
    "        build_11_3 = sub.build_11_3,\n",
    "        km2_1_3 = sub.km2_1_3,\n",
    "        km2_4_10 = sub.km2_4_10,\n",
    "        km2_11_30 = sub.km2_11_30\n",
    "    FROM (\n",
    "        SELECT adm2_pcode,\n",
    "            SUM(CASE WHEN alert_1_3 = 1 THEN pop ELSE 0 END) as pop_1_3,\n",
    "            SUM(CASE WHEN alert_4_10 = 1 THEN pop ELSE 0 END) as pop_4_10,\n",
    "            SUM(CASE WHEN alert_11_30 = 1 THEN pop ELSE 0 END) as pop_11_30,\n",
    "            SUM(CASE WHEN alert_1_3 = 1 THEN bld ELSE 0 END) as build_1_3,\n",
    "            SUM(CASE WHEN alert_4_10 = 1 THEN bld ELSE 0 END) as build_4_10,\n",
    "            SUM(CASE WHEN alert_11_30 = 1 THEN bld ELSE 0 END) as build_11_3,\n",
    "            SUM(CASE WHEN alert_1_3 = 1 THEN km2 ELSE 0 END) as km2_1_3,\n",
    "            SUM(CASE WHEN alert_4_10 = 1 THEN km2 ELSE 0 END) as km2_4_10,\n",
    "            SUM(CASE WHEN alert_11_30 = 1 THEN km2 ELSE 0 END) as km2_11_30\n",
    "        FROM glofas_join\n",
    "        GROUP BY adm2_pcode\n",
    "    ) sub\n",
    "    WHERE a.adm2_pcode = sub.adm2_pcode;\n",
    "    \"\"\")\n",
    "\n",
    "    # SQL query to update data for afg_basin_summary\n",
    "    update_basin_query = text(\"\"\"\n",
    "    UPDATE afg_basin_summary b\n",
    "    SET pop_1_3 = sub.pop_1_3,\n",
    "        pop_4_10 = sub.pop_4_10,\n",
    "        pop_11_30 = sub.pop_11_30,\n",
    "        build_1_3 = sub.build_1_3,\n",
    "        build_4_10 = sub.build_4_10,\n",
    "        build_11_3 = sub.build_11_3,\n",
    "        km2_1_3 = sub.km2_1_3,\n",
    "        km2_4_10 = sub.km2_4_10,\n",
    "        km2_11_30 = sub.km2_11_30\n",
    "    FROM (\n",
    "        SELECT basin_id,\n",
    "            SUM(CASE WHEN alert_1_3 = 1 THEN pop ELSE 0 END) as pop_1_3,\n",
    "            SUM(CASE WHEN alert_4_10 = 1 THEN pop ELSE 0 END) as pop_4_10,\n",
    "            SUM(CASE WHEN alert_11_30 = 1 THEN pop ELSE 0 END) as pop_11_30,\n",
    "            SUM(CASE WHEN alert_1_3 = 1 THEN bld ELSE 0 END) as build_1_3,\n",
    "            SUM(CASE WHEN alert_4_10 = 1 THEN bld ELSE 0 END) as build_4_10,\n",
    "            SUM(CASE WHEN alert_11_30 = 1 THEN bld ELSE 0 END) as build_11_3,\n",
    "            SUM(CASE WHEN alert_1_3 = 1 THEN km2 ELSE 0 END) as km2_1_3,\n",
    "            SUM(CASE WHEN alert_4_10 = 1 THEN km2 ELSE 0 END) as km2_4_10,\n",
    "            SUM(CASE WHEN alert_11_30 = 1 THEN km2 ELSE 0 END) as km2_11_30\n",
    "        FROM glofas_join\n",
    "        GROUP BY basin_id\n",
    "    ) sub\n",
    "    WHERE b.basin_id = sub.basin_id;\n",
    "    \"\"\")\n",
    "\n",
    "    try:\n",
    "        # Execute the update query for afg_basin_summary\n",
    "        conn.execute(update_glofas_join)\n",
    "        conn.execute(update_basin_query)\n",
    "        conn.execute(update_adm2_query)\n",
    "\n",
    "        # Confirmation message\n",
    "        print(\"Glofas_join, Basin and Adm2 summary tables updated successfully\")\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    print(f\"execute_sql_queries end time: {end_time}\")\n",
    "    print(f\"execute_sql_queries Duration: {end_time - start_time}\")\n",
    "\n",
    "# Main Function\n",
    "def getLatestGlofasFlood(date, db_config_path, raster_paths, column_names, directory_path):\n",
    "    config = load_db_config(db_config_path)\n",
    "    db_connection_string = f\"postgresql://{config['username']}:{config['password']}@{config['host']}:{config['port']}/{config['database']}\"\n",
    "\n",
    "    print('Starting Glofas Flood Processing')\n",
    "\n",
    "    # Download the NetCDF file          # OBS blocking out download function for testing\n",
    "    #input_file = download_nc_file(directory_path, date)\n",
    "    input_file = r'D:\\iMMAP\\proj\\ASDC\\data\\GLOFAS\\v02\\glofas_areagrid_for_IMMAP_in_Afghanistan_2023122500.nc'\n",
    "    #input_file = r'D:\\iMMAP\\proj\\ASDC\\data\\GLOFAS\\v02\\glofas_areagrid_for_IMMAP_in_Afghanistan_2023110700_FAKE_QA_VERSION.nc'\n",
    "\n",
    "    # Initialize paths for reference TIFF and output TIFFs\n",
    "    discharge_tif_paths, alert_tif_paths = initialize_paths(directory_path)\n",
    "    \n",
    "    # Read geotransform and projection from reference TIFF\n",
    "    #gt, proj = read_reference_tif(reference_tif_path)\n",
    "    gt = (55.0, 0.05, 0.0, 44.0, 0.0, -0.05)\n",
    "    proj = 'GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]'\n",
    "\n",
    "    # Define the no data value and time ranges\n",
    "    no_data_value = -9999  # Define your no data value\n",
    "    time_ranges = [(0, 3), (3, 10), (10, 30)]\n",
    "\n",
    "    # Process NetCDF data and generate output TIFFs\n",
    "    process_netcdf_data(input_file, time_ranges, discharge_tif_paths, alert_tif_paths, gt, proj, no_data_value)\n",
    "\n",
    "    # Create database connection and perform updates\n",
    "    engine = create_engine(db_connection_string)\n",
    "    with engine.connect() as conn:\n",
    "        glofas_points = gpd.read_postgis('SELECT * FROM glofas_points', conn)\n",
    "        update_glofas_points(conn, raster_paths, column_names, glofas_points)\n",
    "        execute_sql_queries(conn)\n",
    "\n",
    "    print(\"Glofas Flood Processing Completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Glofas Flood Processing\n",
      "process_netcdf_data start time: 2024-01-09 13:53:02.881357\n",
      "save_tif_file start time: 2024-01-09 13:53:19.496273\n",
      "save_tif_file end time: 2024-01-09 13:53:19.553142\n",
      "save_tif_file Duration: 0:00:00.056869\n",
      "create_alert_tif start time: 2024-01-09 13:53:19.554945\n",
      "save_tif_file start time: 2024-01-09 13:53:19.557535\n",
      "save_tif_file end time: 2024-01-09 13:53:19.563751\n",
      "save_tif_file Duration: 0:00:00.006216\n",
      "create_alert_tif end time: 2024-01-09 13:53:19.563751\n",
      "create_alert_tif Duration: 0:00:00.008806\n",
      "save_tif_file start time: 2024-01-09 13:53:20.228448\n",
      "save_tif_file end time: 2024-01-09 13:53:20.261244\n",
      "save_tif_file Duration: 0:00:00.032796\n",
      "create_alert_tif start time: 2024-01-09 13:53:20.265143\n",
      "save_tif_file start time: 2024-01-09 13:53:20.270140\n",
      "save_tif_file end time: 2024-01-09 13:53:20.279719\n",
      "save_tif_file Duration: 0:00:00.009579\n",
      "create_alert_tif end time: 2024-01-09 13:53:20.279719\n",
      "create_alert_tif Duration: 0:00:00.014576\n",
      "save_tif_file start time: 2024-01-09 13:53:22.704655\n",
      "save_tif_file end time: 2024-01-09 13:53:22.745455\n",
      "save_tif_file Duration: 0:00:00.040800\n",
      "create_alert_tif start time: 2024-01-09 13:53:22.748286\n",
      "save_tif_file start time: 2024-01-09 13:53:22.754305\n",
      "save_tif_file end time: 2024-01-09 13:53:22.769360\n",
      "save_tif_file Duration: 0:00:00.015055\n",
      "create_alert_tif end time: 2024-01-09 13:53:22.769360\n",
      "create_alert_tif Duration: 0:00:00.021074\n",
      "process_netcdf_data end time: 2024-01-09 13:53:22.779598\n",
      "process_netcdf_data Duration: 0:00:19.898241\n",
      "update_glofas_points start time: 2024-01-09 13:53:33.388883\n",
      "update_glofas_points end time: 2024-01-09 13:53:37.038962\n",
      "update_glofas_points Duration: 0:00:03.650079\n",
      "execute_sql_queries start time: 2024-01-09 13:53:37.065860\n",
      "Glofas_join, Basin and Adm2 summary tables updated successfully\n",
      "execute_sql_queries end time: 2024-01-09 13:53:39.784143\n",
      "execute_sql_queries Duration: 0:00:02.718283\n",
      "Glofas Flood Processing Completed\n"
     ]
    }
   ],
   "source": [
    "# Celery Task (Example)\n",
    "# from celery import shared_task\n",
    "\n",
    "# @shared_task\n",
    "def run_glofas_flood_task():\n",
    "    \n",
    "    #current_date = datetime.now().date()\n",
    "    #date = current_date.strftime(\"%Y-%m-%d\")\n",
    "    date = \"2024-01-07\"\n",
    "\n",
    "    #db_credential_file = r'D:\\iMMAP\\code\\db_config\\hsdc_local_db_config.json'\n",
    "    db_credential_file = r'D:\\iMMAP\\code\\db_config\\hsdc_live_db_config.json'\n",
    "\n",
    "    raster_paths = [\n",
    "        r'D:\\iMMAP\\proj\\ASDC\\data\\GLOFAS\\v02\\alert_day1_3.tif',\n",
    "        r'D:\\iMMAP\\proj\\ASDC\\data\\GLOFAS\\v02\\alert_day4_10.tif',\n",
    "        r'D:\\iMMAP\\proj\\ASDC\\data\\GLOFAS\\v02\\alert_day11_30.tif'\n",
    "    ]\n",
    "    \n",
    "    column_names = ['alert_1_3', 'alert_4_10', 'alert_11_30']\n",
    "    directory_path = 'D:/iMMAP/proj/ASDC/data/GLOFAS/v02/'\n",
    "    getLatestGlofasFlood(date, db_credential_file, raster_paths, column_names, directory_path)\n",
    "\n",
    "# Uncomment the below line to run the Celery task\n",
    "run_glofas_flood_task()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earth2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
