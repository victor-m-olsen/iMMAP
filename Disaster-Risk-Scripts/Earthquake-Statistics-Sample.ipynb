{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c001e3d",
   "metadata": {},
   "source": [
    "### Earthquake intensity zone statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ab587b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Method ###\n",
    "\n",
    "##Output:\n",
    "# Provinces with population per earthquake risk column. \n",
    "# 5 earthquake risk columns\n",
    "# Shp or spatial database table\n",
    "\n",
    "##Input:\n",
    "# Province polygons\n",
    "# 5 earthquake risk columns\n",
    "# Shp or spatial database table\n",
    "\n",
    "##Input:\n",
    "# Province polygons\n",
    "# Earthquake polyons\n",
    "# Population raster\n",
    "\n",
    "## Steps:\n",
    "\n",
    "# 1.1\n",
    "# Clip province polygons by earthquake polygons\n",
    "# Explode polygons (multipart to single part)\n",
    "\n",
    "# 2.1\n",
    "# Extract population for each polygon using zonal statistics\n",
    "\n",
    "# 2.2\n",
    "# Extract building count for each polygon using spatial join and count\n",
    "\n",
    "# 2.3\n",
    "# Extract area for each polygon\n",
    "\n",
    "# 3.1\n",
    "# Summarize by both province, earthquake, building, area and intensity using pandas pivot table\n",
    "\n",
    "# 4.1\n",
    "# Join pivot table columns to original province polygons x3 (seperate for each dataset)\n",
    "\n",
    "# 5.1\n",
    "# QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0a2f1461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2 # required for exporting to postgis\n",
    "import rioxarray as rxr\n",
    "from rasterio.crs import CRS\n",
    "from sqlalchemy import create_engine\n",
    "import rasterstats\n",
    "from shapely.ops import transform\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03190abd",
   "metadata": {},
   "source": [
    "### Setting connection and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "39b4f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load database configuration from file\n",
    "with open(r'D:\\iMMAP\\code\\db_config\\hsdc_local_db_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Create database URL with credentials\n",
    "db_url = f\"postgresql://{config['username']}:{config['password']}@{config['host']}:{config['port']}/{config['database']}\"\n",
    "\n",
    "# Connect to the database\n",
    "con = create_engine(db_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6fe351f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b8e9da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define projection\n",
    "repro_crs = '+proj=cea'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b91650c0",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "512788be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loading buildings   Start: 09:57:41\n",
      "    Loading buildings   End  : 09:57:42\n"
     ]
    }
   ],
   "source": [
    "pcode = 'adm1_pcode' ### OBS: Adjust based on admin level\n",
    "\n",
    "#pop = r'D:\\iMMAP\\data\\Afghanistan\\Population\\WorldPop\\afg_worldpop_2020_UNadj_unconstrained.tif'\n",
    "#pop = r'C:\\Users\\VMO\\Desktop\\afg_worldpop_2020_UNadj_unconstrained_proj4326.tif'\n",
    "\n",
    "pop = r'D:\\iMMAP\\data\\Afghanistan\\afg_worldpop_2020_UNadj_unconstrained_projCEA.tif' #afg_worldpop_2020_UNadj_unconstrained_projCEA\n",
    "adm = gpd.GeoDataFrame.from_postgis('SELECT * from afg_admbnda_adm1_testclip2', con).to_crs(repro_crs)\n",
    "#adm = gpd.read_file(r\"D:\\iMMAP\\data\\Afghanistan\\HSDC-Official\\afg_admbnda_adm1.shp\")#.to_crs(repro_crs)\n",
    "eq = gpd.read_file(r\"D:\\iMMAP\\data\\Afghanistan\\HSDC-Official\\afg_eq_hzda.shp\").to_crs(repro_crs)\n",
    "\n",
    "print('    Loading buildings   Start: {}'.format(datetime.now().strftime(\"%H:%M:%S\")))\n",
    "build = gpd.GeoDataFrame.from_postgis('SELECT * from afg_buildings_microsoft_centroids_testclip1_tiny', con).to_crs(repro_crs)\n",
    "print('    Loading buildings   End  : {}'.format(datetime.now().strftime(\"%H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b1c1a52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Derived Projected CRS: +proj=cea +type=crs>\n",
       "Name: unknown\n",
       "Axis Info [cartesian]:\n",
       "- E[east]: Easting (metre)\n",
       "- N[north]: Northing (metre)\n",
       "Area of Use:\n",
       "- undefined\n",
       "Coordinate Operation:\n",
       "- name: unknown\n",
       "- method: Lambert Cylindrical Equal Area\n",
       "Datum: World Geodetic System 1984\n",
       "- Ellipsoid: WGS 84\n",
       "- Prime Meridian: Greenwich"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adm.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f63201af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Derived Projected CRS: +proj=cea +type=crs>\n",
       "Name: unknown\n",
       "Axis Info [cartesian]:\n",
       "- E[east]: Easting (metre)\n",
       "- N[north]: Northing (metre)\n",
       "Area of Use:\n",
       "- undefined\n",
       "Coordinate Operation:\n",
       "- name: unknown\n",
       "- method: Lambert Cylindrical Equal Area\n",
       "Datum: World Geodetic System 1984\n",
       "- Ellipsoid: WGS 84\n",
       "- Prime Meridian: Greenwich"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eq.crs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "907d5a70",
   "metadata": {},
   "source": [
    "### 1.1 Clip province polygons by earthquake polygons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "70bd9c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:57:42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VMO\\AppData\\Local\\Temp\\ipykernel_16460\\161707685.py:6: FutureWarning: Currently, index_parts defaults to True, but in the future, it will default to False to be consistent with Pandas. Use `index_parts=True` to keep the current behavior and True/False to silence the warning.\n",
      "  adm_eq = overlay.explode()\n"
     ]
    }
   ],
   "source": [
    "print('{}'.format(datetime.now().strftime(\"%H:%M:%S\")))\n",
    "# Use the overlay function to clip admin polygons by earthquake zones\n",
    "overlay = gpd.overlay(adm, eq, how='intersection', keep_geom_type=None, make_valid=True)\n",
    "\n",
    "# Post-process\n",
    "adm_eq = overlay.explode()\n",
    "adm_eq = adm_eq.to_crs(repro_crs)\n",
    "adm_eq = adm_eq.reset_index()\n",
    "adm_eq = adm_eq.drop(columns=['level_1', 'level_0'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5be495e8",
   "metadata": {},
   "source": [
    "### 2.1 Extract population for each polygon using zonal statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4f1476e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:57:42\n"
     ]
    }
   ],
   "source": [
    "print('{}'.format(datetime.now().strftime(\"%H:%M:%S\")))\n",
    "# Produce list of dictionaries, each dictionary representing the sum for one polygon\n",
    "zonalSt = rasterstats.zonal_stats(adm_eq, pop, stats = 'sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b6d54b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:57:44\n"
     ]
    }
   ],
   "source": [
    "print('{}'.format(datetime.now().strftime(\"%H:%M:%S\")))\n",
    "# Convert list of dictionaries to pandas dataframe (containing one column with each value)\n",
    "df = pd.DataFrame(zonalSt)\n",
    "df = df.rename(columns={'sum': 'pop_eq'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e9b66ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:57:44\n"
     ]
    }
   ],
   "source": [
    "print('{}'.format(datetime.now().strftime(\"%H:%M:%S\")))\n",
    "# Add the values to the original table\n",
    "df_concat = pd.concat([df, adm_eq], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "399d8d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:57:44\n"
     ]
    }
   ],
   "source": [
    "print('{}'.format(datetime.now().strftime(\"%H:%M:%S\")))\n",
    "# Convert table back into a geodataframe    \n",
    "gdf = gpd.GeoDataFrame(df_concat, geometry=df_concat.geometry) #wkb_geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7f2b79f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:57:44\n"
     ]
    }
   ],
   "source": [
    "print('{}'.format(datetime.now().strftime(\"%H:%M:%S\")))\n",
    "# Reorder columns so population sum is last\n",
    "gdf_reordered_columns = gdf[[c for c in gdf if c not in ['pop_eq']] + ['pop_eq']]\n",
    "stats = gdf_reordered_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6a1c255a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:57:44\n"
     ]
    }
   ],
   "source": [
    "print('{}'.format(datetime.now().strftime(\"%H:%M:%S\")))\n",
    "# Set a new ID column\n",
    "stats = stats.reset_index(drop=True)\n",
    "stats = gdf.drop('id', axis=1)\n",
    "stats.insert(0, 'ID', range(len(gdf)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25012604",
   "metadata": {},
   "source": [
    "### 2.2 Extract building count for each polygon using spatial join and count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e97475a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def drop_building_column(df):\n",
    "#    if 'build' in df.columns:\n",
    "#        df.drop('build', axis=1, inplace=True)\n",
    "#    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cfffef85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:57:44\n",
      "    Joining points to polygons   Start: 09:57:44\n",
      "    Joining points to polygons   Finish: 09:57:45\n"
     ]
    }
   ],
   "source": [
    "print('{}'.format(datetime.now().strftime(\"%H:%M:%S\")))\n",
    "# Join building centroids to admin polygons\n",
    "print('    Joining points to polygons   Start: {}'.format(datetime.now().strftime(\"%H:%M:%S\")))\n",
    "\n",
    "points = build\n",
    "polygons = stats\n",
    "\n",
    "#drop_building_column(polygons)\n",
    "\n",
    "# Joining the polygon attributes to each point\n",
    "# Creates a point layer of all buildings with the attributes copied from the interesecting polygon uniquely for each point\n",
    "joined_df = gpd.sjoin(\n",
    "    points,\n",
    "    polygons,\n",
    "    how='inner',\n",
    "    predicate='intersects')\n",
    "\n",
    "print('    Joining points to polygons   Finish: {}'.format(datetime.now().strftime(\"%H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fb8d4137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Counting number of buildings          Start: 09:57:45\n",
      "    Counting number of buildings          End:   09:57:45\n"
     ]
    }
   ],
   "source": [
    "# Count number of buildings within admin polygons (i.e. group by adm code)\n",
    "print('    Counting number of buildings          Start: {}'.format(datetime.now().strftime(\"%H:%M:%S\")))\n",
    "\n",
    "build_count = joined_df.groupby(\n",
    "    ['ID'],\n",
    "    as_index=False,\n",
    ")['geom'].count() # column is arbitrary\n",
    "\n",
    "# Change column name to build_count\n",
    "build_count.rename(columns = {'geom': 'bld_eq'}, inplace = True)\n",
    "\n",
    "print('    Counting number of buildings          End:   {}'.format(datetime.now().strftime(\"%H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6b628ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:57:45\n"
     ]
    }
   ],
   "source": [
    "print('{}'.format(datetime.now().strftime(\"%H:%M:%S\")))\n",
    "# Merge build count back on to admin dataset\n",
    "polygons = polygons.merge(\n",
    "    build_count, \n",
    "    on=['ID'], \n",
    "    how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "946de8ef",
   "metadata": {},
   "source": [
    "### 2.3 Extract area for each polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b5ca9d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:57:45\n"
     ]
    }
   ],
   "source": [
    "print('{}'.format(datetime.now().strftime(\"%H:%M:%S\")))\n",
    "polygons['km2_eq'] = polygons['geometry'].area.div(1000000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92c271f8",
   "metadata": {},
   "source": [
    "### 3.1 Summarize by both province and earthquake intensity using pandas pivot table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ffe6fba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:57:45\n"
     ]
    }
   ],
   "source": [
    "print('{}'.format(datetime.now().strftime(\"%H:%M:%S\")))\n",
    "# pivot table to sum population by admin and intensity\n",
    "pivoted = pd.pivot_table(polygons, values=['pop_eq','bld_eq', 'km2_eq'], index=pcode, columns='intensity', aggfunc='sum')\n",
    "\n",
    "# fill NaN values with 0\n",
    "pivoted.fillna(0, inplace=True)\n",
    "\n",
    "# reset index to make admin a column again\n",
    "pivoted = pivoted.reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd84934d",
   "metadata": {},
   "source": [
    "### 4.1 Join pivot table columns to original province polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b4e343ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:57:45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VMO\\anaconda3\\envs\\earth2\\lib\\site-packages\\geopandas\\geodataframe.py:1470: FutureWarning: merging between different levels is deprecated and will be removed in a future version. (1 levels on the left, 2 on the right)\n",
      "  result = DataFrame.merge(self, *args, **kwargs)\n",
      "c:\\Users\\VMO\\anaconda3\\envs\\earth2\\lib\\site-packages\\geopandas\\geodataframe.py:1470: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  result = DataFrame.merge(self, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "print('{}'.format(datetime.now().strftime(\"%H:%M:%S\")))\n",
    "eq_stats = adm.merge(\n",
    "            pivoted, \n",
    "            on=pcode, \n",
    "            how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5843c107",
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_stats.rename(columns=lambda x: x[0] + '_' + str(x[1]) if isinstance(x, tuple) and isinstance(x[1], int) else x, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d37328f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1_eq_stats = eq_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a6c5429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which is the first pop_eq column that exists in the dataset\n",
    "\n",
    "def get_first_column_startswith(df, prefix):\n",
    "    for column in df.columns:\n",
    "        if column.startswith(prefix):\n",
    "            return column\n",
    "    return None\n",
    "\n",
    "prefix_pop = 'pop_eq'\n",
    "prefix_build = 'bld_eq'\n",
    "prefix_km2 = 'km2_eq'\n",
    "\n",
    "first_column_pop = get_first_column_startswith(eq_stats, prefix_pop)\n",
    "first_column_build = get_first_column_startswith(eq_stats, prefix_build)\n",
    "first_column_km2 = get_first_column_startswith(eq_stats, prefix_km2)\n",
    "\n",
    "#print(f\"First column starting with '{prefix}': {first_column_pop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bb18f9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:57:46\n",
      "Converted population figures\n",
      "Converted building figures\n",
      "Rounded area figures\n"
     ]
    }
   ],
   "source": [
    "print('{}'.format(datetime.now().strftime(\"%H:%M:%S\")))\n",
    "# Convert pop figures to int\n",
    "eq_stats.loc[:,first_column_pop:'pop_eq_10'] = eq_stats.loc[:,first_column_pop:'pop_eq_10'].astype(int)\n",
    "print('Converted population figures')\n",
    "\n",
    "# Convert building figures to int\n",
    "eq_stats.loc[:,first_column_build:'bld_eq_10'] = eq_stats.loc[:,first_column_build:'bld_eq_10'].astype(int)\n",
    "print('Converted building figures')\n",
    "\n",
    "# Round area figures to 2 decimal places\n",
    "eq_stats.loc[:,first_column_km2:'km2_eq_10'] = eq_stats.loc[:,first_column_km2:'km2_eq_10'].round(1)\n",
    "print('Rounded area figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c4e098c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:57:46\n"
     ]
    }
   ],
   "source": [
    "print('{}'.format(datetime.now().strftime(\"%H:%M:%S\")))\n",
    "# Reprojet back to 4326\n",
    "eq_stats = eq_stats.to_crs('epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "359506c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:57:46\n"
     ]
    }
   ],
   "source": [
    "print('{}'.format(datetime.now().strftime(\"%H:%M:%S\")))\n",
    "# Save output to database\n",
    "eq_stats.to_postgis('afg_admbnda_eq_hzda_stats_v12', con, if_exists='replace')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7c5c3de",
   "metadata": {},
   "source": [
    "### 5.1 QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "df40c154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adm1 = gpd.GeoDataFrame.from_postgis('SELECT * from afg_admbnda_eq_hzda_stats_v06', con)\n",
    "#adm2 = gpd.GeoDataFrame.from_postgis('SELECT * from afg_admbnda_eq_hzda_stats_v07', con)\n",
    "#region = gpd.GeoDataFrame.from_postgis('SELECT * from afg_admbnda_eq_hzda_stats_v08', con)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18aabd14",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "25e7e235",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adm1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\GitHub\\iMMAP\\DRR-Statistics\\Earthquake-Statistics-Subset-V03.ipynb Cell 41\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/GitHub/iMMAP/DRR-Statistics/Earthquake-Statistics-Subset-V03.ipynb#Y113sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m adm \u001b[39m=\u001b[39m adm1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GitHub/iMMAP/DRR-Statistics/Earthquake-Statistics-Subset-V03.ipynb#Y113sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mbuild_count total\u001b[39m\u001b[39m'\u001b[39m, adm\u001b[39m.\u001b[39mbuild\u001b[39m.\u001b[39msum())\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GitHub/iMMAP/DRR-Statistics/Earthquake-Statistics-Subset-V03.ipynb#Y113sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mbuild_count eq\u001b[39m\u001b[39m'\u001b[39m, adm\u001b[39m.\u001b[39mloc[:, [\u001b[39m'\u001b[39m\u001b[39mbld_eq_5\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbld_eq_6\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbld_eq_7\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbld_eq_8\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbld_eq_9\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbld_eq_10\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39msum())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'adm1' is not defined"
     ]
    }
   ],
   "source": [
    "adm = adm1\n",
    "print('build_count total', adm.build.sum())\n",
    "print('build_count eq', adm.loc[:, ['bld_eq_5', 'bld_eq_6', 'bld_eq_7', 'bld_eq_8', 'bld_eq_9', 'bld_eq_10']].sum().sum())\n",
    "print('pop_sum total', adm.pop.sum())\n",
    "print('pop_sum eq', adm.loc[:,['pop_eq_5', 'pop_eq_6', 'pop_eq_7', 'pop_eq_8', 'pop_eq_9', 'pop_eq_10']].sum().sum())\n",
    "print('area total', adm.km2.sum())\n",
    "print('area eq', adm.loc[:,['km2_eq_5', 'km2_eq_6', 'km2_eq_7', 'km2_eq_8', 'km2_eq_9', 'km2_eq_10']].sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a15030",
   "metadata": {},
   "outputs": [],
   "source": [
    "adm = adm2\n",
    "print('build_count', adm.build_ls_4.sum())\n",
    "print('build_count all', adm.loc[:, ['bld_eq_5', 'bld_eq_6', 'bld_eq_7', 'bld_eq_8', 'bld_eq_9', 'bld_eq_10']].sum().sum())\n",
    "print('pop_sum', adm.pop_sum.sum())\n",
    "print('pop_sum - excluding 0', adm.loc[:,['pop_eq_5', 'pop_eq_6', 'pop_eq_7', 'pop_eq_8', 'pop_eq_9', 'pop_eq_10']].sum().sum())\n",
    "print('area_km2', adm.km2_ls_4.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f4017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adm = region\n",
    "print('build_count', adm.build_ls_4.sum())\n",
    "print('build_count all', adm.loc[:, ['bld_eq_5', 'bld_eq_6', 'bld_eq_7', 'bld_eq_8', 'bld_eq_9', 'bld_eq_10']].sum().sum())\n",
    "print('pop_sum', adm.pop_sum.sum())\n",
    "print('pop_sum - excluding 0', adm.loc[:,['pop_eq_5', 'pop_eq_6', 'pop_eq_7', 'pop_eq_8', 'pop_eq_9', 'pop_eq_10']].sum().sum())\n",
    "print('area_km2', adm.km2_ls_4.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb781d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
